---
title: "Розділ 4 Лабораторна робота №3. Розвідувальний аналіз даних. Візуалізація"
author: "[Daniil Tereshchenko](https://www.linkedin.com/in/daniil-tereshchenko/), `r format(Sys.time(), '%Y')`"
date: "`r Sys.Date()`"
output: 
  cleanrmd::html_document_clean:
    theme: axist
bibliography: references_lab.bib
---

### Варіація

__Варіація (Variation)__ -- це тенденція до змін значення змінної від вимірювання до вимірювання. Ми можете легко помітити варіації в реальному житті; якщо ми двічі вимірюємо постійну змінну, ми отримаємо два різні результати.  
Категоричні змінні також можуть відрізнятися, якщо виміри робити для різних суб'єктів (наприклад, кольори очей різних людей) або у різні моменти часу (наприклад, енергетичні рівні електрона в різні моменти). Кожна змінна має свій власний паттерн у варіації, який може виявити цікаву інформацію. Найкращий спосіб зрозуміти цю закономірність -- візуалізувати розподіл значень змінної.  

#### Візуалізація розподілу

Нижче наведено приклад розподілу категоріальної змінної.

```{r message=FALSE, warning=FALSE, include=FALSE, paged.print=FALSE}
packageNeed <- c("knitr", "dplyr", "ggplot2", "devtools", "sparklyr",
                 "GGally", "corrplot", "PerformanceAnalytics", "FactoMineR",
                 "factoextra", "funModeling", "desctable", "ade4", "psych",
                 "smacof", "WVPlots", "caret", "car")
lapply(packageNeed, library, character.only = TRUE)
```

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(tidyverse)
library(ggplot2)
ggplot(data = diamonds) +
  geom_bar(mapping = aes(x = cut))
```

Частоту для кожного значення категоріальної змінної можна обчислита, наприклад, так:

```{r}
diamonds %>%
  count(cut)
```

Для неперервної змінної доцільно побудувати гістограму:

```{r}
ggplot(data = diamonds) +
  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)
```

Інтервальна таблиця частот, що відповідає гістограмі, може бути обчислена так:

```{r}
diamonds %>%
  count(cut_width(carat, 0.5))
```

Можна побудувати гістограму для певної долі значень:

```{r}
smaller <- diamonds %>% 
  filter(carat < 3)
  
ggplot(data = smaller, mapping = aes(x = carat)) +
  geom_histogram(binwidth = 0.1)
```

Часто буває доцільно побудувати серію гістограм для різних груп спостережень:

```{r}
ggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +
  geom_freqpoly(binwidth = 0.1)
```

Після того, як ми виконали візуалізацію, що ми маємо знайти на цих графіках? Яка може бути послідовність запитань на наступному етапі?  

*Типові запитання з урахуванням специфіка даної задачі можуть виглядати так:

* Які значення є найбільш поширеними? Чому?
* Які значення є рідкісними? Чому? Це відповідає нашим очікуванням?
* Чи бачемо ми якісь незвичайні закономірності? Що може їх пояснити?

Як приклад, гістограма нижче наводить кілька цікавих питань:

* Чому там більше діамантів праворуч від кожного піка, ніж трохи ліворуч від кожного піка?
* Чому немає діамантів більше 3 каратів?

```{r}
ggplot(data = smaller, mapping = aes(x = carat)) + 
         geom_histogram(binwidth = 0.01)
```

#### Незвичайні значення

Як правило у вибіркових даних зустірчаються викиди (outliers) – такі значення свідчать або про похибку вимірювання, або про якість надзвичайні причини, що потребують уважного вивчення.

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5)
```

Для того, щоб їх побачити, необхідно певним чином масштабувати гістограму:

```{r}
ggplot(diamonds) + 
  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +
  coord_cartesian(ylim = c(0, 50))
```

Якщо приймається рішення їх видалити, то це можна зроити наступним чином:

```{r}
unusual <- diamonds %>%
  filter(y < 3 | y > 20) %>% 
  select(price, x,y,z) %>%
  arrange(y)
unusual
```

#### Пропущені значення (Missing values)

Часто на практиці дані виявляються некомплектними -- мають місце пропущенні дані (`NA`). У таких випадках відомі два виходи з систуації:

* видалити некомплектні спостереження
* виконати імпутацію пропущених значень -- замінити пропущені значення певними у відповідності з якимось алгоритмом.

Пакет `ggplot2` автоматично видаляє некмоплектні дані: 

```{r}
diamonds
```

```{r}
diamonds2 <- diamonds %>%
  mutate(y = ifelse( y < 3 | y > 20, NA, y))
```

```{r}
ggplot(data = diamonds2, mapping = aes(x = x, y = y)) + 
  geom_point()
```

Інколи ми хочемо зрозуміти, що робить спостереження з відсутніми значеннями, відмінними від спостережень із записаними значеннями. Наприклад, у `nycflights13::flights`, відсутні значення в змінній `dep_time` (час вильоту) показують, що рейс був скасований. Тому, можливо, нам потрібно буде порівняти заплановані терміни вильоту для скасованих та не скасованих часів. Ми можемо зробити це, зробивши нову змінну з `is.na()`:

```{r}
nycflights13::flights %>% 
  mutate(
    cancelled = is.na(dep_time),
    sched_hour = sched_dep_time %/% 100,
    sched_min = sched_dep_time %% 100,
    sched_dep_time = sched_hour + sched_min / 60
  ) %>% 
  ggplot(mapping = aes(sched_dep_time)) + 
    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)
```

### Коваріація

Якщо варіація описує поведінку в межах змінної, коваріація описує поведінку між змінними.  
__Коваріація (Covariation)__ -- це схильність значень двох чи більше змінних змінюватися разом. Найкращим способом виявлення коваріації є візуалізація відносин між двома чи більше змінних.

#### Категоріальні та неперервні змінні

Природнім є рішення щодо вивчення розподілу неперервної змінної, розбивши її на групи у відповідності до значень категоріальної змінної, як у попередньому багатокутнику частот. Поява за замовчуванням `geom_freqpoly()` не є таким корисним для подібного порівняння, оскільки його висоту визначає кількість. Це означає, що якщо одна з груп є набагато меншою, ніж інші, важко побачити відмінності у формі. Наприклад, давайте розглянемо, як ціна діамантів змінюється залежно від його якості:

```{r}
ggplot(data = diamonds, mapping = aes(x = price)) +
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

Важко побачити різницю в розподілах, оскільки кількість вибіркових значень у кожній групі суттєво відрізняється:

```{r}
ggplot(diamonds) +
  geom_bar(mapping = aes(x = cut))
```

Для полегшення порівняння нам потрібно поміняти те, що відображається на осі Y. Замість того, щоб відображати частоту, ми покажемо відносну частоту, яка є нормованою величиною.

```{r}
ggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) +
  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)
```

В результаті можна побачити, що найвищу середню ціну мають посередні діаманти (fair). Але аналіз щільностей розпоілів є не зовсім зручним. Альтернативним варіантом представлення аналогічної інформації є п’ятиквантильний графік (boxplot, box and wiskers plot), відомий як “боксплот,” або “ящик з вусами.” Боксплот акумулює в собі всі найважливіші інтегральні харакетристики стосовно мір центральної тенденції, розсіювання та форми розподілу.Тоді для нашого випадку застосування боксплотів дасть такий результат:

```{r}
ggplot(data = diamonds, mapping = aes(x = cut, y = price)) +
  geom_boxplot()
```

#### Дві категоріальні змінні

Для візуалізації коваріації між категоріальними змінними неохідно візуалізувати частоти: у вигляді таблиці, або певного графічного візуалізатора. Наприклад:

```{r}
ggplot(data = diamonds) +
  geom_count(mapping = aes(x = cut, y = color) )
```

З результатів видно, що існує певна залежність між кольором діаманта, та якістю його ограненості.

#### Пакет `FunModelling`

Відносно недавно з'явився пакет під назвою `FunModelling` [@R-funModeling], в арсеналі якого є набір корисних інструментів, що суттєво спрощують деякі процедури розвідувального аналізу, особливо на самому початку, коли вивчається структура даних. Зокрема

* `df_status()` : структура набору даних для профілювання
* `describe()` : чисельне та категоріальне профілювання (кількісне)
* `freq()`: категоріальне профілювання (кількісне та графік).
* `profileing_num()`: профілювання для числових змінних (кількісний)
* `plot_num (дані)` : профілювання для числових змінних (графіки)


### Зниження розмірності даних

__[Зниження розмірності (Dimensionality reduction)](https://en.wikipedia.org/wiki/Dimensionality_reduction)__ -- процесс скорочення кількості випадкових змінних шляхом отримання гооловних змінних. Цей процес можно поділити _обирання ознак_ та _виділяння ознак_.  
_Обирання ознак_ -- це процес пошуку первісних змінних (факторів), що починаються в рамках фази розуміння даних із залученням експертів предметної галузі і з залученням всього арсеналу інстурментів маніпулювання даними, про що йшлося вище. (Маніпулятивні методики).    
_Проектування ознак_ -- це перетворення даних з багатовимірного простору у простір простір невеликої кількості вимірів. (Математичні методики). (Далі під зниженням розмірнонсті будем мати на увазі саме проектування ознак). Існує велика кількість лінійних і нелінійних методик зниження розмірності.  
Що дає зниження розмірності на практиці? В першу чергу спрощення представлення багатовимірних даних, їх візуалізацію, вирішення задач класифікації та регресії і, власне, краще розуміння процесів, що моделюються.  
Одним з фундаментальних лінійних методів зниження розмірності, що широко викорстовується на практиці, є [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA].  


#### Постановка задачі аналізу  методом главних компонент (PCA) 

Припустимо [@PCA], що ми маємо випадковий вектор $X$:

$$
X=\begin{pmatrix}
x_1 \\
x_2 \\
\vdots \\
x_n
\end{pmatrix}
$$
З коваріаційною матрицею:

$$
var(X) = \Sigma = 
\begin{pmatrix}
\sigma_{1}^2 & \sigma_{12} & \ldots & \sigma_{1p}\\
\sigma_{21} & \sigma_{2}^2 & \ldots & \sigma_{2p}\\
\vdots & \vdots & \ddots & \vdots\\
\sigma_{p1} & \sigma_{p2} & \ldots & \sigma_{p}^2
\end{pmatrix}
$$

Мета [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) [@PCA_b] полягає в пошуку $k$ лінійних комбінацій $p$ змінних $X$, що містять найбільшу дисперсію. Лінійна комбінація має насупний вигляд:

$Y_1 = a_{11} X_1 + a_{12} X_2 + \cdots + a_{1p} X_p$ 

$Y_2 = a_{21} X_1 + a_{22} X_2 + \cdots + a_{2p} X_p$

$\vdots$

$Y_k = a_{k1} X_1 + a_{k2} X_2 + \cdots + a_{kp} X_p$


При цьому $\sum\limits_{i=1}^pa_{1i}^2=1$ і т. д.

Дисперсия першої главної компоненти $var(Y_1)=a_1'\Sigma a_1$, де $\Sigma$ -- ковариаційна матриця.

Аналогічно обчислюється дисперсія другої і т. д. головних компонент.

У даній моделі вектори $a_i'=(a_{i1}, a_{i2},...,a_{ip})',\;i=\overline{1,p}$ представляють _власні вектори_ ковариаційної матриці $\Sigma$, тоді як дисперсія $i$-ої головної компоненти дорівнює _власному значенню_ матриці ковариацій:

$var(Y_i)=\lambda_i$.

_Загальна дисперсія_ вибірки дорівнює $\sum\limits_{i=1}^p\lambda_{i}.$  
Метод головних компонент (PCA) вирішуючи задачу зниження розмірності дозволяє одночасно вирішити задачу сегементації (кластеризації) -- тобто з'ясувати, чи є досліджувані дані однорідними, чи сегментовані на групи зі схожими ознаками. Відповідь на це питання є однією з головних задач експлораторного аналізу, що передує етапу побуви більш складних моделей класифікації, регресії чи моделей на основі асоціативних правил.    

В арсеналі R існує багато інструментів для реалізації зниження розмірності, зокрема PCA.  

# Виконання індивідуального завдання

Дані з досліджуваного набору мають наступний вигляд:

```{r}
iris %>%
  head()
```

Обчислимо і дослідимо сумарні статистики.

```{r}
iris %>%
  df_status()
```

Що ми бачимо? 

* всі дані мають числову природу
* дані комплектні: відстуні пропущені значення
* відсутні нульові значення -- це знімає можливі проблеми при трансформації
* відсутні надвелики значення -- надвелики значення кожен раз потребують серйозної уваги і аналізу можливих причин, що їх викликали
* по всіх змінних дані мають варіацію по унікальним значенням одного порядку.  

Дослідимо закони розподілу кожної з чотирьох змінних.


```{r}
iris %>%
  plot_num()
```

```{r}
iris %>%
  profiling_num()
```

Що ми бачимо?  

* статистичні розподіли змінних `"Sepal.Length"`, `"Sepal.Width"` мають дзвоноподібну форму, наближену до нормального. Враховуючи, що значення оцінок асимметрії та ексцесу несуттєво відрізняються від нуля, в першому наближенні можна вважати дані розподіли нормальними. Про що це говорить і що це дає? По-перше, це говорить про те, що доля малих і великих даних врівноважують одна одну, по-друге, нормальність законів розподілу досліджуваних величин, або, принаймні, "натяк" на нормальність __завжди добре__, тому що класичними передумовами для коректної побудови великої кількості різного роду моделей вимагає від даних нормального закону розподілу, чи, принаймні, симетричності закону розподілу. В нашому випадку це є передумовою однорідного розподілу спостережень у просторі інформативних ознак, що є позитивним моментом при вирішенні задачі сегментації.  
* статистичні розподіли змінних `"Petal.Length"`, `"Petal.Width"` на відміну від двох інших, мають чітку бімодальну структуру, що гооврить про явно виражену неоднорідність даних і про те, що саме ці дві змінні є дискримінуючими у просторі досліджуваних ознак; це важливо для побудови задачі сегментації

Для відповіді на питання, чи пов'язані між собою змінні, застосуємо кореляційний аналіз. З урахуванням числової природи даних, для оцінки кореляції скористаємося коефіцієнтом кореляції Пірсона. Враховуючі багатомірний аналіз початкових даних, важливо вдало підібрати візуалізатор. Нижче запропоновано два з найбільш відомих і поширених.

```{r}
iris %>%
  select(-Species) %>%
      cor() %>%
  corrplot(order = "hclust", tl.col = 'black', tl.cex = .75)
```

```{r}
iris %>%
  select(-Species) %>%
  pairs(main = "Edgar Anderson's Iris Data", font.main = 4, pch = 19, col = iris$Species)
```

```{r}
df_iris <- iris %>%
  select(-Species) 

df_iris %>%
  cor() %>%
  knitr::kable(caption = "Таблиця оцінок коефіцієнтів кореляції")
```

Що ми бачимо? 

* має місце сильний позитивний кореляційний зв'язок між змінною `Sepal.Length` та змінними  `Petal.Length`, `Petal.Width`; на кореляційних полях чітко видно наявність даної кореляції
* на кореляційних полях чітко видно наявність неоднородності даних -- дані чітко поділяються на гомогенні групи за змінною `Species`
* має місце слабкий від'ємний кореляційний зв'язок `Petal.Length` і `Sepal.Width`; дана кореляція є уявною в силу сегментованості даних по змінній `Species`: якщо уважно дослідити форму кореляційних полів для кожного значення данної змінної, то можна побачити, що всередині кожного сегменту має місце позитивна кореляція

Що це нам дає?  

* наявність високого ступеня кореляції дає можливість знизити розмірність даних і знайти просту структуру у просторі меншої розмірності 
* у просторі меншої розмірності можна можна виконати сегментацію даних.

Для зниження розмірності і одночасно сегментації даних скористаємося методом головних компонент (PCA).

```{r}
#PCA
resPCA <- iris %>%
  select(-Species) %>%
  PCA(ncp = 8, graph = FALSE)
```

```{r}
eigenvalues <- as.data.frame(resPCA$eig)
cumVar <- round(eigenvalues$`cumulative percentage of variance`[length(eigenvalues$eigenvalue[eigenvalues$eigenvalue >= 0.9])], 2)
```

```{r}
knitr::kable(
  eigenvalues, 
  caption = "Власні значення (eigenvalues) і сумарний процент поясненої дисперсії"
)
```

```{r}
fviz_screeplot(resPCA, addlabels = TRUE,  ncp=10)
```

Чщо ми бачимо?  
Ми маємо $p=$ `r length(eigenvalues$eigenvalu[eigenvalues$eigenvalue >= 0.9])` головних компонент, які пояснюють `r cumVar` % дисперсії. Це значить, що м маємо всього дві нові компоненти замість чотирьох і практично без втрати інформації можемо представити всі спостереження в системі двох координат на площині: перша компонента по осі `Х`, друга -- по осі Y (див. рис.).  
Проаналізуємо детально структуру двох перших компонент, виключивши решту незначимих (див. табл. і рис.).

```{r}
knitr::kable(
  resPCA$var$coord[ ,1:2], 
  caption = "Таблиця навантажень"
)
```

Що ми бачимо?

* Структуру першої компоненти головним чином складають три початкові змінні: `Sepal.Length`,  `Petal.Length`, `Petal.Width`; як і прогнозувалося раніше, саме за цією компонентою відбувається дискримінація (розрізнення) трьох різних сегментів трьох типів ірисів
* основне навантаження на другу компоненту складає змінна `Sepal.Width` -- всі три види ірисів можуть мати досить велику варіацію за цим параметром.

```{r}
# Biplot of individuals and variables
fviz_pca_biplot(resPCA,
                geom = c("point"),
                # label = "none", # hide individual labels
             habillage = as.factor(iris$Species), # color by groups
             axes = c(1, 2),
             repel = TRUE,
             label = c("ind", "ind.sup", "quali", "var", "quanti.sup"),
             select.var = list(name = c("Sepal.Length", "Sepal.Width", "Petal.Length", "Petal.Width")),
             # select.var = list(contrib = 8),
             # label = c("ind.sup"),
             palette = c("#00AFBB", "#E7B800", "#FC4E07", "#00AFBB", "#E7B800", "#FC4E07"),
             # alpha.var = c("contrib"),
             # col.ind = c("contrib"),
             # col.ind.sup = c("contrib"),
             addEllipses = TRUE # Concentration ellipses
             ) +
  theme_minimal()
```

Таким чином, з’ясовано, шо початкові дані не є однорідними. Три типи ірисів різняться за довжинами внутрішніх часток оцвітини (petal length) та шириною внутрішньої частки оцвітини (petal width). Завдяки наявності кореляцій у початкових змінних, спостереження вдалося добре описати у просторі двох інтегральних показників. Знайдені кластери характерні для трьох типів ірисів і у майбутньому можуть бути використані для написання класифікатора з метою розпізнавання нових об’єктів.

## References

Casas, Pablo. 2020. funModeling: Exploratory Data Analysis and Data Preparation Tool-Box. [https://livebook.datascienceheroes.com](https://livebook.datascienceheroes.com).

Garrett Grolemund, Hadley Wickham. 2018. R for Data Science. [http://r4ds.had.co.nz/index.html](http://r4ds.had.co.nz/index.html).

Science, Eberly College of. 2020. “STAT 505: Applied Multivariate Statistical Analysis.” Course materials. [http://belousovv.ru/markdown_syntax#fnref:2](http://belousovv.ru/markdown_syntax#fnref:2).
wikipedia. 
2018a. “Cross-Industry Standard Process for Data Mining.” Article. [https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining](https://en.wikipedia.org/wiki/Cross-industry_standard_process_for_data_mining).

2018b. “Exploratory Data Analysis.” Article. [https://en.wikipedia.org/wiki/Exploratory_data_analysis](https://en.wikipedia.org/wiki/Exploratory_data_analysis).

2018c. “Principal Component Analysis.” Article. [https://en.wikipedia.org/wiki/Principal_component_analysis](https://en.wikipedia.org/wiki/Principal_component_analysis).




